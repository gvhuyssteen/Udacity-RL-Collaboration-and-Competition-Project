{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from collections import OrderedDict, deque\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Tennis.app')\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, fc1_units, fc2_units):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.tanh(self.fc3(x))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, fc1_units, fc2_units):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            fc1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        xs = F.relu(self.fc1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = [128, 64]\n",
    "max_memory = 100000\n",
    "batch_size = 128\n",
    "learning_rate = 0.0001\n",
    "tau = 0.001\n",
    "gamma = 0.99\n",
    "update_every = 1\n",
    "\n",
    "class Agent():\n",
    "    \n",
    "    def __init__(self, state_size, action_size, checkpoint = None):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.actor_local = Actor(state_size, action_size, hidden_layers[0], hidden_layers[1])\n",
    "        self.actor_target = Actor(state_size, action_size, hidden_layers[0], hidden_layers[1])\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.critic_local = Critic(state_size, action_size, hidden_layers[0], hidden_layers[1])\n",
    "        self.critic_target = Critic(state_size, action_size, hidden_layers[0], hidden_layers[1])\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=learning_rate)\n",
    "        \n",
    "        if checkpoint is not None:\n",
    "            self.actor_local.load_state_dict(checkpoint['actor_local'])\n",
    "            self.actor_target.load_state_dict(checkpoint['actor_target'])\n",
    "            self.critic_local.load_state_dict(checkpoint['critic_local'])\n",
    "            self.critic_target.load_state_dict(checkpoint['critic_target'])\n",
    "\n",
    "        self.memory = deque(maxlen=max_memory)\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def get_action(self, state, add_noise=True):\n",
    "        \n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        \n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            actions = self.actor_local(state)\n",
    "        self.actor_local.train()\n",
    "        \n",
    "        return actions.detach().numpy()\n",
    "\n",
    "    def train(self, state, action, reward, next_state, done):\n",
    "        self.memory.append({\n",
    "            \"state\": state,\n",
    "            \"action\": action,\n",
    "            \"reward\": reward,\n",
    "            \"next_state\": next_state,\n",
    "            \"done\": done\n",
    "        })\n",
    "        \n",
    "        self.t_step = (self.t_step + 1) % update_every\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory) > batch_size:\n",
    "                experiences = random.sample(self.memory, k = batch_size)\n",
    "                \n",
    "                states = torch.from_numpy(np.vstack([e['state'] for e in experiences if e is not None])).float()\n",
    "                actions = torch.from_numpy(np.vstack([e['action'] for e in experiences if e is not None])).float()\n",
    "                rewards = torch.from_numpy(np.vstack([e['reward'] for e in experiences if e is not None])).float()\n",
    "                next_states = torch.from_numpy(np.vstack([e['next_state'] for e in experiences if e is not None])).float()\n",
    "                dones = torch.from_numpy(np.vstack([e['done'] for e in experiences if e is not None]).astype(np.uint8)).float()\n",
    "                \n",
    "                actions_next = self.actor_target(next_states)\n",
    "                Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "                Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "                Q_expected = self.critic_local(states, actions)\n",
    "                critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                self.critic_optimizer.step()\n",
    "\n",
    "                actions_pred = self.actor_local(states)\n",
    "                actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "                self.soft_update(self.critic_local, self.critic_target, tau)\n",
    "                self.soft_update(self.actor_local, self.actor_target, tau)                     \n",
    "\n",
    "                \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, Average -0.005999999716877937\n",
      "Episode 200, Average 0.0030000004172325134\n",
      "Episode 300, Average -0.009999999776482582\n",
      "Episode 400, Average -0.009999999776482582\n",
      "Episode 500, Average -0.009999999776482582\n",
      "Episode 600, Average -0.00799999974668026\n",
      "Episode 700, Average -0.009999999776482582\n",
      "Episode 800, Average -0.009999999776482582\n",
      "Episode 900, Average 0.022000000700354575\n",
      "Episode 1000, Average 0.053000001162290575\n",
      "Episode 1100, Average 0.06700000137090684\n",
      "Episode 1200, Average 0.2792000045441091\n",
      "Episode 1300, Average 0.13800000242888927\n",
      "Episode 1400, Average 0.20510000344365836\n",
      "Episode 1500, Average 0.14400000251829626\n",
      "Episode 1600, Average 0.3023000048659742\n",
      "Episode 1700, Average 0.13100000232458114\n",
      "Episode 1800, Average 0.10000000186264515\n",
      "Episode 1900, Average 0.1010000018775463\n",
      "Episode 2000, Average 0.11200000204145909\n",
      "Episode 2100, Average 0.26930000437423585\n",
      "Solved in 2196 episodes with a score of 0.509600007943809\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint_filename = 'checkpoint.pth'\n",
    "number_of_episodes = 10000\n",
    "\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "state = env_info.vector_observations[0]\n",
    "state_size = len(state)\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "agent = Agent(state_size, action_size)\n",
    "\n",
    "all_scores = []\n",
    "scores = deque(maxlen=100)\n",
    "\n",
    "state_dict_score = 0\n",
    "\n",
    "if os.path.exists(checkpoint_filename):\n",
    "    checkpoint = torch.load(checkpoint_filename)\n",
    "    if checkpoint:\n",
    "        state_dict_score = checkpoint['score']\n",
    "\n",
    "for episode in range(1, number_of_episodes + 1):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    episode_score = 0\n",
    "\n",
    "    while True:\n",
    "        actions = []\n",
    "        for i in range(num_agents):\n",
    "            action = agent.get_action(states[i])\n",
    "            actions.append(action)\n",
    "            \n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "\n",
    "        for j in range(num_agents):\n",
    "            agent.train(states[j], actions[j], rewards[j], next_states[j], dones[j])\n",
    "        \n",
    "            # Save scores\n",
    "            episode_score += rewards[j]\n",
    "            \n",
    "        if np.any(dones):\n",
    "            break\n",
    "        \n",
    "        states = next_states\n",
    "        \n",
    "\n",
    "    scores.append(episode_score)\n",
    "    all_scores.append(episode_score)\n",
    "    average_score = np.mean(scores)\n",
    "    \n",
    "    if episode % 100 == 0:\n",
    "        print('Episode {}, Average {}'.format(episode, average_score))\n",
    "    \n",
    "    if average_score > state_dict_score:\n",
    "        state_dict_score = average_score\n",
    "        torch.save({\n",
    "            \"score\": average_score,\n",
    "            \"actor_local\": agent.actor_local.state_dict(),\n",
    "            \"actor_target\": agent.actor_target.state_dict(),\n",
    "            \"critic_local\": agent.critic_local.state_dict(),\n",
    "            \"critic_target\": agent.critic_target.state_dict()\n",
    "        }, checkpoint_filename)\n",
    "        \n",
    "    if average_score >= 0.5:\n",
    "        print('Solved in {} episodes with a score of {}'.format(episode, average_score))\n",
    "        break\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "plt.plot(np.arange(len(all_scores)), all_scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint with score of  0.509600007943809\n"
     ]
    }
   ],
   "source": [
    "checkpoint_filename = 'checkpoint.pth'\n",
    "checkpoint = torch.load(checkpoint_filename)\n",
    "\n",
    "print('Loading checkpoint with score of ', checkpoint['score'])\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "states = env_info.vector_observations\n",
    "state_size = len(states[0])\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "agent = Agent(state_size, action_size, checkpoint)\n",
    "\n",
    "while True:\n",
    "    \n",
    "    actions = []\n",
    "    for i in range(num_agents):\n",
    "        action = agent.get_action(states[i], False)\n",
    "        if i == 0:\n",
    "            actions = action\n",
    "        else:\n",
    "            actions = np.append(actions, action, axis = 0)\n",
    "    \n",
    "    env_info = env.step(actions)[brain_name]\n",
    "    next_states = env_info.vector_observations\n",
    "    reward = env_info.rewards\n",
    "    dones = env_info.local_done\n",
    "    \n",
    "    if np.any(dones):\n",
    "        break\n",
    "            \n",
    "    states = next_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To close the unity environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
